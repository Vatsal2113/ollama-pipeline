name: Ollama Pipeline with SageMaker

on:
  push:
    branches: [ main, add-pipeline-with-sagemaker1 ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-and-run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      # Create scripts directory if it doesn't exist
      - name: Create scripts directory
        run: mkdir -p scripts/training_scripts
      
      # Make all script files executable
      - name: Make script files executable
        run: |
          chmod +x pipeline.sh
          chmod +x extract_gguf.py
          chmod +x sagemaker_finetune.py
          chmod +x scripts/quantize.py
          chmod +x scripts/create_modelfile.py
          chmod +x scripts/training_scripts/train.py
      
      # Use Docker CLI's built-in compose functionality
      - name: Build and run with Docker Compose
        run: |
          # Start Ollama service first
          docker compose up -d ollama
          
          # Give Ollama time to initialize
          echo "Waiting for Ollama to initialize..."
          sleep 30
          
          # Run the pipeline container
          docker compose up pipeline
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          MODELS_TO_PULL: "llama2 mistral"
          FINETUNE_MODEL: "mistralai/Mistral-7B-v0.1"
          TRAINING_DATA: "s3://ollama-lora-pipeline/training-data/"