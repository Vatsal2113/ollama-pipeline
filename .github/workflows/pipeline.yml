name: Ollama Model Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Name for the final model'
        default: 'tinyllama-custom'
        required: true
      base_model:
        description: 'Base model to fine-tune'
        default: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
        required: true
      instance_type:
        description: 'SageMaker instance type'
        default: 'ml.g4dn.xlarge'
        required: false

jobs:
  train-convert-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-2' }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 sagemaker huggingface-hub datasets torch peft

      - name: Create LoRA Training Scripts
        run: |
          mkdir -p ./scripts/training_scripts/lora_trainer
          
          # Create a simple LoRA training script
          cat > ./scripts/training_scripts/lora_trainer/train_lora.py << 'EOF'
          #!/usr/bin/env python3

          import os
          import argparse
          import logging
          import sys
          import traceback
          import torch
          from datasets import load_dataset

          logging.basicConfig(
              level=logging.INFO,
              format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
              handlers=[logging.StreamHandler(sys.stdout)]
          )
          logger = logging.getLogger(__name__)

          def find_training_files():
              """Find training data files in SageMaker paths."""
              base_dir = "/opt/ml/input/data/training"
              logger.info(f"Looking for training data in: {base_dir}")
              
              if not os.path.exists(base_dir):
                  logger.error(f"Directory not found: {base_dir}")
                  # Create sample data as fallback
                  logger.info("Creating sample training data")
                  os.makedirs("/tmp/sample_data", exist_ok=True)
                  with open("/tmp/sample_data/sample.jsonl", "w") as f:
                      f.write('{"text": "This is a sample text for training."}\n')
                      f.write('{"text": "Here is another example of training data."}\n')
                  return ["/tmp/sample_data/sample.jsonl"]

              files = []
              for filename in os.listdir(base_dir):
                  if filename.endswith('.json') or filename.endswith('.jsonl'):
                      files.append(os.path.join(base_dir, filename))
                      logger.info(f"Found training file: {filename}")
              
              if not files:
                  logger.warning("No JSON/JSONL files found in training directory")
                  # Create sample data
                  sample_path = os.path.join(base_dir, "sample.jsonl")
                  with open(sample_path, "w") as f:
                      f.write('{"text": "This is a sample text for training."}\n')
                      f.write('{"text": "Here is another example of training data."}\n')
                  files.append(sample_path)
                  logger.info(f"Created sample data at: {sample_path}")
              
              return files

          def main():
              try:
                  # Print environment information first
                  logger.info(f"Python version: {sys.version}")
                  logger.info(f"PyTorch version: {torch.__version__}")
                  logger.info(f"CUDA available: {torch.cuda.is_available()}")
                  if torch.cuda.is_available():
                      logger.info(f"CUDA device: {torch.cuda.get_device_name(0)}")
                      logger.info(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB")
                  
                  # Import libraries after logging versions
                  try:
                      import transformers
                      logger.info(f"Transformers version: {transformers.__version__}")
                      from transformers import (
                          AutoModelForCausalLM,
                          AutoTokenizer,
                          Trainer,
                          TrainingArguments,
                          DataCollatorForLanguageModeling
                      )
                  except ImportError as e:
                      logger.error(f"Failed to import transformers: {e}")
                      raise
                      
                  try:
                      import peft
                      logger.info(f"PEFT version: {peft.__version__}")
                      from peft import (
                          get_peft_model,
                          LoraConfig,
                          TaskType
                      )
                  except ImportError as e:
                      logger.error(f"Failed to import peft: {e}")
                      raise
                      
                  try:
                      import bitsandbytes as bnb
                      logger.info(f"BitsAndBytes version: {bnb.__version__}")
                  except ImportError:
                      logger.warning("BitsAndBytes not available, will not use 8-bit optimization")
                      bnb = None
                  
                  # Parse arguments
                  parser = argparse.ArgumentParser(description="Train a language model with LoRA")
                  
                  # Required parameters from SageMaker
                  parser.add_argument("--model_name_or_path", type=str, required=True)
                  parser.add_argument("--output_dir", type=str, required=True)
                  
                  # Optional parameters
                  parser.add_argument("--per_device_train_batch_size", type=int, default=2)
                  parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
                  parser.add_argument("--learning_rate", type=float, default=2e-4)
                  parser.add_argument("--num_train_epochs", type=int, default=1)
                  
                  # LoRA parameters
                  parser.add_argument("--use_lora", type=str, default='True')
                  parser.add_argument("--lora_r", type=int, default=8)
                  parser.add_argument("--lora_alpha", type=int, default=16)
                  parser.add_argument("--lora_dropout", type=float, default=0.05)
                  
                  args = parser.parse_args()
                  
                  # Log all arguments
                  logger.info(f"Arguments: {args}")
                  logger.info(f"Training model: {args.model_name_or_path}")
                  
                  # Find training files
                  training_files = find_training_files()
                  logger.info(f"Using training files: {training_files}")
                  
                  # Load dataset
                  try:
                      dataset = load_dataset("json", data_files=training_files, split="train")
                      logger.info(f"Loaded dataset with {len(dataset)} examples")
                      if len(dataset) > 0:
                          logger.info(f"Sample: {dataset[0]}")
                  except Exception as e:
                      logger.error(f"Error loading dataset: {e}")
                      raise
                  
                  # Load tokenizer
                  try:
                      logger.info(f"Loading tokenizer: {args.model_name_or_path}")
                      tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
                      if tokenizer.pad_token is None:
                          tokenizer.pad_token = tokenizer.eos_token
                          logger.info("Set pad_token to eos_token")
                  except Exception as e:
                      logger.error(f"Error loading tokenizer: {e}")
                      raise
                  
                  # Tokenize function
                  def tokenize(examples):
                      return tokenizer(
                          examples["text"], 
                          truncation=True, 
                          padding="max_length", 
                          max_length=128
                      )
                  
                  # Process dataset
                  try:
                      logger.info("Tokenizing dataset...")
                      tokenized_dataset = dataset.map(tokenize, batched=True)
                      logger.info(f"Dataset tokenized successfully")
                  except Exception as e:
                      logger.error(f"Error tokenizing dataset: {e}")
                      raise
                  
                  # Load model - Use 8-bit quantization to reduce memory footprint
                  try:
                      logger.info(f"Loading model: {args.model_name_or_path}")
                      
                      # Check if bitsandbytes is available
                      load_in_8bit = False
                      if bnb is not None:
                          load_in_8bit = True
                          logger.info("Using 8-bit quantization")
                      
                      model = AutoModelForCausalLM.from_pretrained(
                          args.model_name_or_path,
                          load_in_8bit=load_in_8bit,
                          device_map="auto" if torch.cuda.is_available() else None
                      )
                      logger.info(f"Model loaded successfully")
                  except Exception as e:
                      logger.error(f"Error loading model: {e}")
                      logger.error(traceback.format_exc())
                      raise
                  
                  # Set up LoRA if enabled
                  if args.use_lora.lower() == 'true':
                      try:
                          logger.info("Setting up LoRA")
                          
                          # Simple target modules that work well for most models
                          target_modules = ["q_proj", "v_proj"]
                          
                          logger.info(f"Using target modules for LoRA: {target_modules}")
                          
                          lora_config = LoraConfig(
                              r=args.lora_r,
                              lora_alpha=args.lora_alpha,
                              target_modules=target_modules,
                              lora_dropout=args.lora_dropout,
                              bias="none",
                              task_type=TaskType.CAUSAL_LM
                          )
                          
                          model = get_peft_model(model, lora_config)
                          logger.info("LoRA applied to model successfully")
                      except Exception as e:
                          logger.error(f"Error setting up LoRA: {e}")
                          logger.error(traceback.format_exc())
                          raise
                  
                  # Set up data collator
                  data_collator = DataCollatorForLanguageModeling(
                      tokenizer=tokenizer, 
                      mlm=False
                  )
                  
                  # Training arguments
                  try:
                      training_args = TrainingArguments(
                          output_dir=args.output_dir,
                          per_device_train_batch_size=args.per_device_train_batch_size,
                          gradient_accumulation_steps=args.gradient_accumulation_steps,
                          learning_rate=args.learning_rate,
                          num_train_epochs=args.num_train_epochs,
                          save_strategy="epoch",
                          logging_steps=10,
                          remove_unused_columns=False
                      )
                  except Exception as e:
                      logger.error(f"Error setting up training arguments: {e}")
                      raise
                  
                  # Initialize trainer
                  try:
                      trainer = Trainer(
                          model=model,
                          args=training_args,
                          train_dataset=tokenized_dataset,
                          data_collator=data_collator,
                          tokenizer=tokenizer
                      )
                  except Exception as e:
                      logger.error(f"Error initializing trainer: {e}")
                      raise
                  
                  # Train model
                  try:
                      logger.info("Starting training...")
                      trainer.train()
                      logger.info("Training completed successfully!")
                  except Exception as e:
                      logger.error(f"Error during training: {e}")
                      logger.error(traceback.format_exc())
                      raise
                  
                  # Save model
                  try:
                      logger.info(f"Saving model to {args.output_dir}")
                      model.save_pretrained(args.output_dir)
                      tokenizer.save_pretrained(args.output_dir)
                      logger.info(f"Model saved successfully")
                  except Exception as e:
                      logger.error(f"Error saving model: {e}")
                      raise
                  
              except Exception as e:
                  logger.error(f"Unhandled error: {str(e)}")
                  logger.error(traceback.format_exc())
                  sys.exit(1)

          if __name__ == "__main__":
              main()
          EOF
          
          # Create requirements.txt file with stable versions
          cat > ./scripts/training_scripts/lora_trainer/requirements.txt << 'EOF'
          transformers==4.26.0
          datasets>=2.9.0
          torch==1.13.1
          peft==0.3.0
          bitsandbytes>=0.37.0
          accelerate>=0.16.0
          scipy
          scikit-learn
          EOF

      - name: Create SageMaker Fine-tuning Script
        run: |
          cat > sagemaker_finetune.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import argparse
          import logging
          import boto3
          import sagemaker
          import time
          import string
          import random
          from sagemaker.huggingface import HuggingFace

          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
          )
          logger = logging.getLogger("sagemaker_finetune")

          def setup_args():
              """Parse command line arguments."""
              parser = argparse.ArgumentParser(description="Fine-tune a model with SageMaker")
              
              parser.add_argument(
                  "--base-model",
                  required=True,
                  help="Base model to fine-tune"
              )
              
              parser.add_argument(
                  "--training-data",
                  required=True,
                  help="S3 URI of training data"
              )
              
              parser.add_argument(
                  "--output-bucket",
                  required=True,
                  help="S3 bucket for output model"
              )
              
              parser.add_argument(
                  "--instance-type",
                  default="ml.g4dn.xlarge",
                  help="SageMaker training instance type"
              )
              
              return parser.parse_args()

          def main():
              args = setup_args()
              try:
                  logger.info(f"Setting up SageMaker finetuning for {args.base_model}")
                  
                  # Set up SageMaker session
                  sagemaker_session = sagemaker.Session()
                  role = os.environ.get("SAGEMAKER_ROLE_ARN")
                  logger.info(f"Using role: {role}")
                  
                  # Define hyperparameters for LoRA fine-tuning
                  hyperparameters = {
                      'model_name_or_path': args.base_model,
                      'output_dir': '/opt/ml/model',
                      'per_device_train_batch_size': 1,  # Reduced batch size
                      'gradient_accumulation_steps': 4,  # Increased gradient accumulation
                      'learning_rate': 2e-4,
                      'num_train_epochs': 1,
                      
                      # LoRA specific parameters
                      'use_lora': 'True',
                      'lora_r': 8,
                      'lora_alpha': 16,
                      'lora_dropout': 0.05
                  }
                  
                  # Generate a simple job name
                  timestamp = int(time.time())
                  suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))
                  job_name = f"train-{suffix}"
                  
                  # Use Hugging Face estimator with well-tested stable versions
                  huggingface_estimator = HuggingFace(
                      entry_point='train_lora.py',
                      source_dir='./scripts/training_scripts/lora_trainer',
                      instance_type=args.instance_type,
                      instance_count=1,
                      role=role,
                      transformers_version='4.26.0',  # More stable version
                      pytorch_version='1.13.1',       # Known to work well with SageMaker
                      py_version='py39',              # Compatible Python version
                      hyperparameters=hyperparameters,
                      debugger_hook_config=False      # Disable SageMaker debugger
                  )
                  
                  # Create training job inputs
                  inputs = {'training': args.training_data}
                  
                  # Start the training job
                  logger.info(f"Starting training job: {job_name}")
                  huggingface_estimator.fit(inputs=inputs, job_name=job_name)
                  logger.info(f"Training job {job_name} completed")
                  
                  # Get the model artifacts
                  model_data = huggingface_estimator.model_data
                  logger.info(f"Model artifacts: {model_data}")
                  
              except Exception as e:
                  logger.error(f"Error during fine-tuning: {str(e)}")
                  exit(1)

          if __name__ == "__main__":
              main()
          EOF
          chmod +x sagemaker_finetune.py

      - name: Construct S3 URI and Fine-tune Model
        id: finetune
        env:
          SAGEMAKER_ROLE_ARN: ${{ secrets.AWS_SAGEMAKER_ROLE_ARN }}
          TRAINING_INSTANCE_TYPE: ${{ github.event.inputs.instance_type || 'ml.g4dn.xlarge' }}
          S3_BUCKET_NAME: "ollama-lora-pipeline"
        run: |
          # Use the exact path to the training data folder, not a specific file
          TRAINING_DATA_URI="s3://${S3_BUCKET_NAME}/training-data/"
          echo "Using training data from: $TRAINING_DATA_URI"
          echo "Using instance type: $TRAINING_INSTANCE_TYPE"
          
          # Run the SageMaker fine-tuning script
          python sagemaker_finetune.py \
            --base-model "${{ github.event.inputs.base_model || 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' }}" \
            --training-data "$TRAINING_DATA_URI" \
            --output-bucket "${S3_BUCKET_NAME}" \
            --instance-type "$TRAINING_INSTANCE_TYPE"
          
          # Get the training job name
          JOB_NAME=$(aws sagemaker list-training-jobs --sort-by "CreationTime" --sort-order "Descending" --max-items 1 --query "TrainingJobSummaries[0].TrainingJobName" --output text)
          echo "TRAINING_JOB_NAME=${JOB_NAME}" >> $GITHUB_ENV

      - name: Download Fine-tuned Model
        run: |
          python scripts/download_model.py --job-name $TRAINING_JOB_NAME --output-dir ./fine-tuned-model

      - name: Merge LoRA Weights
        run: |
          python scripts/merge_lora.py --base-model "${{ github.event.inputs.base_model || 'TinyLlama/TinyLlama-1.1B-Chat-v1.0' }}" --lora-model ./fine-tuned-model --output-dir ./merged-model

      - name: Convert to GGUF Format
        run: |
          mkdir -p ./models
          python scripts/convert_to_gguf.py --input-dir ./merged-model --output-file ./models/${{ github.event.inputs.model_name || 'tinyllama-custom' }}.gguf --outtype q4_k_m

      - name: Create Ollama Modelfile
        run: |
          python scripts/create_modelfile.py --gguf-path ./models/${{ github.event.inputs.model_name || 'tinyllama-custom' }}.gguf --model-name ${{ github.event.inputs.model_name || 'tinyllama-custom' }}

      - name: Evaluate Model
        id: evaluate
        run: |
          python scripts/evaluate_model.py --model-dir ./merged-model --output-file ./evaluation_results.json
          echo "eval_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Upload to S3
        env:
          S3_BUCKET_NAME: "ollama-lora-pipeline"
        run: |
          MODEL_NAME=${{ github.event.inputs.model_name || 'tinyllama-custom' }}
          
          # Create timestamp for versioning
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          
          # Create directories in S3 if they don't exist
          aws s3api put-object --bucket $S3_BUCKET_NAME --key models/${MODEL_NAME}/${TIMESTAMP}/
          aws s3api put-object --bucket $S3_BUCKET_NAME --key models/${MODEL_NAME}/latest/
          
          # Upload GGUF model
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/${MODEL_NAME}.gguf
          
          # Upload Modelfile
          aws s3 cp ./Modelfile s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/Modelfile
          
          # Upload evaluation results
          aws s3 cp ./evaluation_results.json s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/evaluation_results.json
          
          # Update "latest" pointer
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/${MODEL_NAME}.gguf
          aws s3 cp ./Modelfile s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/Modelfile
          aws s3 cp ./evaluation_results.json s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/evaluation_results.json
          
          echo "Model artifacts uploaded to: s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/"
          echo "Latest model available at: s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/"

      - name: Generate Report
        run: |
          python scripts/generate_report.py --eval-file ./evaluation_results.json --output-file ./report.md

      - name: Upload Evaluation Results
        uses: actions/upload-artifact@v4
        with:
          name: model-evaluation
          path: |
            ./evaluation_results.json
            ./report.md

      - name: Add Evaluation Summary Comment
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const reportContent = fs.readFileSync('./report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: reportContent
            });