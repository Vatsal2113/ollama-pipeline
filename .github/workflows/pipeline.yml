name: Ollama Model Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Name for the final model'
        default: 'mistral-custom'
        required: true
      base_model:
        description: 'Base model to fine-tune'
        default: 'mistralai/Mistral-3B-Instruct-v0.2'
        required: true
      instance_type:
        description: 'SageMaker instance type'
        default: 'ml.g4dn.xlarge'
        required: false

jobs:
  train-convert-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-2' }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create Training Scripts Directory
        run: |
          mkdir -p ./scripts/training_scripts
          
          # Create simplified QLoRA script
          cat > ./scripts/training_scripts/run_qlora_simple.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import sys
          import json
          import traceback
          import argparse
          import torch
          from transformers import (
              AutoModelForCausalLM,
              AutoTokenizer,
              Trainer,
              TrainingArguments,
              BitsAndBytesConfig,
              default_data_collator,
              set_seed,
          )
          from datasets import load_dataset
          from peft import (
              LoraConfig,
              prepare_model_for_kbit_training,
              get_peft_model,
          )

          def log_error(e):
              print(f"ERROR: {str(e)}")
              print(f"Error type: {type(e).__name__}")
              print(f"Traceback:\n{''.join(traceback.format_tb(e.__traceback__))}")
              
              # Print system info for debugging
              print(f"Python version: {sys.version}")
              print(f"PyTorch version: {torch.__version__}")
              print(f"CUDA available: {torch.cuda.is_available()}")
              if torch.cuda.is_available():
                  print(f"CUDA version: {torch.version.cuda}")
                  print(f"GPU: {torch.cuda.get_device_name(0)}")
                  print(f"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB")
                  print(f"Free GPU memory: {torch.cuda.memory_reserved(0) / 1e9} GB used")

          def find_jsonl_files():
              """Find all JSONL files in the SageMaker environment."""
              base_dir = "/opt/ml/input/data"
              print(f"Searching for training data in: {base_dir}")
              
              # List all available directories
              all_dirs = []
              try:
                  all_dirs = os.listdir(base_dir)
                  print(f"Available directories: {all_dirs}")
              except Exception as e:
                  print(f"Error listing {base_dir}: {str(e)}")
              
              # Search for JSONL files
              jsonl_files = []
              for dir_name in all_dirs:
                  dir_path = os.path.join(base_dir, dir_name)
                  if os.path.isdir(dir_path):
                      try:
                          files = os.listdir(dir_path)
                          print(f"Files in {dir_path}: {files}")
                          
                          # Find all JSON/JSONL files
                          for file in files:
                              if file.endswith('.json') or file.endswith('.jsonl'):
                                  file_path = os.path.join(dir_path, file)
                                  jsonl_files.append(file_path)
                                  print(f"Found JSON/JSONL file: {file_path}")
                      except Exception as e:
                          print(f"Error listing {dir_path}: {str(e)}")
              
              # If no files found, create a sample file
              if not jsonl_files:
                  print("No JSON/JSONL files found. Creating sample data...")
                  sample_path = "/tmp/sample_data.json"
                  with open(sample_path, 'w') as f:
                      for i in range(10):
                          f.write(json.dumps({
                              "text": f"This is a sample training example {i}."
                          }) + "\n")
                  jsonl_files = [sample_path]
                  print(f"Created sample data at {sample_path}")
              
              return jsonl_files

          def preprocess_function(examples, tokenizer, max_length=512):
              """Tokenize the input text."""
              return tokenizer(
                  examples["text"],
                  truncation=True,
                  max_length=max_length,
                  padding="max_length",
                  return_tensors=None,
              )

          def main():
              try:
                  # Parse arguments
                  parser = argparse.ArgumentParser()
                  parser.add_argument("--model_name_or_path", type=str)
                  parser.add_argument("--output_dir", type=str)
                  parser.add_argument("--use_4bit", type=lambda x: x.lower() == 'true', default=True)
                  parser.add_argument("--bnb_4bit_compute_dtype", type=str, default="float16")
                  parser.add_argument("--bnb_4bit_quant_type", type=str, default="nf4")
                  parser.add_argument("--use_nested_quant", type=lambda x: x.lower() == 'true', default=True)
                  parser.add_argument("--num_train_epochs", type=int, default=3)
                  parser.add_argument("--per_device_train_batch_size", type=int, default=1)
                  parser.add_argument("--gradient_accumulation_steps", type=int, default=4)
                  parser.add_argument("--learning_rate", type=float, default=2e-5)
                  parser.add_argument("--fp16", type=lambda x: x.lower() == 'true', default=True)
                  parser.add_argument("--save_strategy", type=str, default="steps")
                  parser.add_argument("--save_steps", type=int, default=500)
                  parser.add_argument("--optim", type=str, default="paged_adamw_8bit")
                  args = parser.parse_args()
                  
                  # Set random seed
                  set_seed(42)
                  
                  # Print versions and available GPU memory
                  print(f"Python version: {sys.version}")
                  print(f"PyTorch version: {torch.__version__}")
                  print(f"CUDA available: {torch.cuda.is_available()}")
                  if torch.cuda.is_available():
                      print(f"CUDA version: {torch.version.cuda}")
                      print(f"GPU: {torch.cuda.get_device_name(0)}")
                      print(f"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9} GB")
                  
                  # Find training data
                  jsonl_files = find_jsonl_files()
                  
                  # Load dataset
                  print(f"Loading dataset from {len(jsonl_files)} files...")
                  raw_dataset = load_dataset("json", data_files=jsonl_files, split="train")
                  print(f"Loaded dataset with {len(raw_dataset)} examples")
                  
                  # Print a sample
                  if len(raw_dataset) > 0:
                      print(f"Dataset sample: {raw_dataset[0]}")
                  
                  # Load tokenizer first to tokenize the dataset
                  print(f"Loading tokenizer from {args.model_name_or_path}")
                  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
                  if tokenizer.pad_token is None:
                      tokenizer.pad_token = tokenizer.eos_token
                      
                  # Tokenize the dataset
                  print("Tokenizing dataset...")
                  tokenized_dataset = raw_dataset.map(
                      lambda examples: preprocess_function(examples, tokenizer),
                      batched=True,
                      remove_columns=["text"]
                  )
                  print(f"Tokenized dataset size: {len(tokenized_dataset)}")
                  
                  # Set up quantization configuration
                  compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)
                  print(f"Using compute dtype: {compute_dtype}")
                  
                  bnb_config = BitsAndBytesConfig(
                      load_in_4bit=args.use_4bit,
                      bnb_4bit_quant_type=args.bnb_4bit_quant_type,
                      bnb_4bit_compute_dtype=compute_dtype,
                      bnb_4bit_use_double_quant=args.use_nested_quant,
                  )
                  
                  # Load base model with quantization
                  print(f"Loading model {args.model_name_or_path} with 4-bit quantization...")
                  model = AutoModelForCausalLM.from_pretrained(
                      args.model_name_or_path,
                      quantization_config=bnb_config,
                      device_map="auto",
                      trust_remote_code=True,
                      torch_dtype=torch.float16
                  )
                  
                  # Prepare model for k-bit training
                  model = prepare_model_for_kbit_training(model)
                  
                  # Set up LoRA configuration
                  # For Mistral, target modules should include attention layers
                  target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
                  
                  # For some models we might need to detect the right target modules
                  if "mistral" in args.model_name_or_path.lower():
                      print("Using Mistral-specific target modules")
                  elif "llama" in args.model_name_or_path.lower():
                      print("Using LLaMA-specific target modules")
                      target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
                  
                  print(f"LoRA target modules: {target_modules}")
                  
                  lora_config = LoraConfig(
                      r=16,
                      lora_alpha=32,
                      lora_dropout=0.05,
                      bias="none",
                      task_type="CAUSAL_LM",
                      target_modules=target_modules
                  )
                  
                  # Apply LoRA to model
                  model = get_peft_model(model, lora_config)
                  model.print_trainable_parameters()
                  
                  # Set up training arguments
                  training_args = TrainingArguments(
                      output_dir=args.output_dir,
                      per_device_train_batch_size=args.per_device_train_batch_size,
                      gradient_accumulation_steps=args.gradient_accumulation_steps,
                      learning_rate=args.learning_rate,
                      num_train_epochs=args.num_train_epochs,
                      fp16=args.fp16,
                      save_strategy=args.save_strategy,
                      save_steps=args.save_steps,
                      logging_steps=10,
                      optim=args.optim,
                      lr_scheduler_type="cosine",
                      warmup_ratio=0.05,
                      report_to="none"
                  )
                  
                  # Initialize trainer with a simple data collator
                  trainer = Trainer(
                      model=model,
                      args=training_args,
                      train_dataset=tokenized_dataset,
                      data_collator=default_data_collator,
                      tokenizer=tokenizer,
                  )
                  
                  # Train!
                  print("Starting training...")
                  trainer.train()
                  
                  # Save model
                  print(f"Saving model to {args.output_dir}")
                  model.save_pretrained(args.output_dir)
                  tokenizer.save_pretrained(args.output_dir)
                  
                  print("Training completed successfully!")
              
              except Exception as e:
                  log_error(e)
                  sys.exit(1)

          if __name__ == "__main__":
              main()
          EOF
          
          # Create requirements.txt with compatible versions
          cat > ./scripts/training_scripts/requirements.txt << 'EOF'
          transformers==4.28.1
          accelerate==0.20.3
          peft==0.4.0
          bitsandbytes==0.40.2
          datasets==2.12.0
          scipy
          scikit-learn
          sentencepiece
          EOF

      - name: Construct S3 URI and Fine-tune Model
        id: finetune
        env:
          SAGEMAKER_ROLE_ARN: ${{ secrets.AWS_SAGEMAKER_ROLE_ARN }}
          TRAINING_INSTANCE_TYPE: ${{ github.event.inputs.instance_type || 'ml.g4dn.xlarge' }}
          S3_BUCKET_NAME: "ollama-lora-pipeline"
        run: |
          # Use the exact path to the training data folder, not a specific file
          TRAINING_DATA_URI="s3://${S3_BUCKET_NAME}/training-data/"
          echo "Using training data from: $TRAINING_DATA_URI"
          echo "Using instance type: $TRAINING_INSTANCE_TYPE"
          
          # Run the SageMaker fine-tuning script
          python sagemaker_finetune.py \
            --base-model "${{ github.event.inputs.base_model || 'mistralai/Mistral-3B-Instruct-v0.2' }}" \
            --training-data "$TRAINING_DATA_URI" \
            --output-bucket "${S3_BUCKET_NAME}" \
            --instance-type "$TRAINING_INSTANCE_TYPE"
          
          # Get the training job name
          JOB_NAME=$(aws sagemaker list-training-jobs --sort-by "CreationTime" --sort-order "Descending" --max-items 1 --query "TrainingJobSummaries[0].TrainingJobName" --output text)
          echo "TRAINING_JOB_NAME=${JOB_NAME}" >> $GITHUB_ENV

      - name: Download Fine-tuned Model
        run: |
          python scripts/download_model.py --job-name $TRAINING_JOB_NAME --output-dir ./fine-tuned-model

      - name: Merge LoRA Weights
        run: |
          python scripts/merge_lora.py --base-model "${{ github.event.inputs.base_model || 'mistralai/Mistral-3B-Instruct-v0.2' }}" --lora-model ./fine-tuned-model --output-dir ./merged-model

      - name: Convert to GGUF Format
        run: |
          mkdir -p ./models
          python scripts/convert_to_gguf.py --input-dir ./merged-model --output-file ./models/${{ github.event.inputs.model_name || 'mistral-custom' }}.gguf --outtype q4_k_m

      - name: Create Ollama Modelfile
        run: |
          python scripts/create_modelfile.py --gguf-path ./models/${{ github.event.inputs.model_name || 'mistral-custom' }}.gguf --model-name ${{ github.event.inputs.model_name || 'mistral-custom' }}

      - name: Evaluate Model
        id: evaluate
        run: |
          python scripts/evaluate_model.py --model-dir ./merged-model --output-file ./evaluation_results.json
          echo "eval_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Upload to S3
        env:
          S3_BUCKET_NAME: "ollama-lora-pipeline"
        run: |
          MODEL_NAME=${{ github.event.inputs.model_name || 'mistral-custom' }}
          
          # Create timestamp for versioning
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          
          # Create directories in S3 if they don't exist
          aws s3api put-object --bucket $S3_BUCKET_NAME --key models/${MODEL_NAME}/${TIMESTAMP}/
          aws s3api put-object --bucket $S3_BUCKET_NAME --key models/${MODEL_NAME}/latest/
          
          # Upload GGUF model
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/${MODEL_NAME}.gguf
          
          # Upload Modelfile
          aws s3 cp ./Modelfile s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/Modelfile
          
          # Upload evaluation results
          aws s3 cp ./evaluation_results.json s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/evaluation_results.json
          
          # Update "latest" pointer
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/${MODEL_NAME}.gguf
          aws s3 cp ./Modelfile s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/Modelfile
          aws s3 cp ./evaluation_results.json s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/evaluation_results.json
          
          echo "Model artifacts uploaded to: s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/"
          echo "Latest model available at: s3://${S3_BUCKET_NAME}/models/${MODEL_NAME}/latest/"

      - name: Generate Report
        run: |
          python scripts/generate_report.py --eval-file ./evaluation_results.json --output-file ./report.md

      - name: Upload Evaluation Results
        uses: actions/upload-artifact@v4
        with:
          name: model-evaluation
          path: |
            ./evaluation_results.json
            ./report.md

      - name: Add Evaluation Summary Comment
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const reportContent = fs.readFileSync('./report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: reportContent
            });