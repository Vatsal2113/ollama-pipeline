name: Ollama Model Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Name for the final model'
        default: 'mistral-custom'
        required: true
      base_model:
        description: 'Base model to fine-tune'
        default: 'mistralai/Mistral-7B-v0-1'
        required: true
      training_data:
        description: 'Path to training data in S3'
        default: 's3://your-bucket/data/train.jsonl'
        required: true
      instance_type:
        description: 'SageMaker instance type'
        default: 'ml.p3.2xlarge'
        required: false

jobs:
  train-convert-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create Training Scripts Directory
        run: |
          mkdir -p ./scripts/training_scripts
          
          # Create train.py script
          cat > ./scripts/training_scripts/train.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import torch
          from transformers import (
              AutoModelForCausalLM,
              AutoTokenizer,
              Trainer,
              TrainingArguments,
              default_data_collator,
          )
          from datasets import load_dataset
          from peft import LoraConfig, get_peft_model
          
          def main():
              # Get hyperparameters from environment variables
              model_id = os.environ["model_name_or_path"]
              
              # Load dataset
              train_dataset = load_dataset("json", data_files=os.environ["SM_CHANNEL_TRAINING"], split="train")
              
              # Load model and tokenizer
              model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
              tokenizer = AutoTokenizer.from_pretrained(model_id)
              
              # Apply LoRA configuration
              lora_config = LoraConfig(
                  r=8,
                  lora_alpha=16,
                  target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
                  lora_dropout=0.05,
                  bias="none",
                  task_type="CAUSAL_LM"
              )
              model = get_peft_model(model, lora_config)
              model.print_trainable_parameters()
              
              # Training arguments
              training_args = TrainingArguments(
                  output_dir=os.environ["output_dir"],
                  per_device_train_batch_size=int(os.environ.get("per_device_train_batch_size", 1)),
                  gradient_accumulation_steps=int(os.environ.get("gradient_accumulation_steps", 4)),
                  learning_rate=float(os.environ.get("learning_rate", 2e-5)),
                  num_train_epochs=int(os.environ.get("num_train_epochs", 3)),
                  fp16=os.environ.get("fp16", "True").lower() == "true",
                  save_strategy=os.environ.get("save_strategy", "steps"),
                  save_steps=int(os.environ.get("save_steps", 500)),
              )
              
              # Initialize trainer
              trainer = Trainer(
                  model=model,
                  args=training_args,
                  train_dataset=train_dataset,
                  data_collator=default_data_collator,
              )
              
              # Train model
              trainer.train()
              
              # Save trained model and tokenizer
              model.save_pretrained(os.environ["output_dir"])
              tokenizer.save_pretrained(os.environ["output_dir"])
          
          if __name__ == "__main__":
              main()
          EOF
          
          # Create requirements.txt
          cat > ./scripts/training_scripts/requirements.txt << 'EOF'
          transformers>=4.49.0
          accelerate>=0.30.0
          peft>=0.9.0
          bitsandbytes>=0.43.0
          trl>=0.7.11
          datasets>=2.19.0
          torch>=2.5.0
          EOF

      - name: Fine-tune Model with SageMaker
        id: finetune
        env:
          SAGEMAKER_ROLE_ARN: ${{ secrets.AWS_SAGEMAKER_ROLE_ARN }}
          TRAINING_INSTANCE_TYPE: ${{ github.event.inputs.instance_type || 'ml.p3.2xlarge' }}
        run: |
          # Run the SageMaker fine-tuning script
          python sagemaker_finetune.py \
            --base-model "${{ github.event.inputs.base_model || 'mistralai/Mistral-7B-v0-1' }}" \
            --training-data "${{ github.event.inputs.training_data }}" \
            --output-bucket "${{ secrets.S3_BUCKET_NAME }}" \
            --instance-type "${{ github.event.inputs.instance_type || 'ml.p3.2xlarge' }}"
          
          # Get the training job name
          JOB_NAME=$(aws sagemaker list-training-jobs --sort-by "CreationTime" --sort-order "Descending" --max-items 1 --query "TrainingJobSummaries[0].TrainingJobName" --output text)
          echo "TRAINING_JOB_NAME=${JOB_NAME}" >> $GITHUB_ENV

      - name: Download Fine-tuned Model
        run: |
          python scripts/download_model.py --job-name $TRAINING_JOB_NAME --output-dir ./fine-tuned-model

      - name: Merge LoRA Weights
        run: |
          python scripts/merge_lora.py --base-model "${{ github.event.inputs.base_model || 'mistralai/Mistral-7B-v0-1' }}" --lora-model ./fine-tuned-model --output-dir ./merged-model

      - name: Convert to GGUF Format
        run: |
          mkdir -p ./models
          python scripts/convert_to_gguf.py --input-dir ./merged-model --output-file ./models/${{ github.event.inputs.model_name || 'mistral-custom' }}.gguf --outtype q4_k_m

      - name: Create Ollama Modelfile
        run: |
          python scripts/create_modelfile.py --gguf-path ./models/${{ github.event.inputs.model_name || 'mistral-custom' }}.gguf --model-name ${{ github.event.inputs.model_name || 'mistral-custom' }}

      - name: Evaluate Model
        id: evaluate
        run: |
          python scripts/evaluate_model.py --model-dir ./merged-model --output-file ./evaluation_results.json
          echo "eval_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Upload to S3
        run: |
          MODEL_NAME=${{ github.event.inputs.model_name || 'mistral-custom' }}
          BUCKET_NAME="${{ secrets.S3_BUCKET_NAME }}"
          
          # Create timestamp for versioning
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          
          # Create directories in S3 if they don't exist
          aws s3api put-object --bucket $BUCKET_NAME --key models/${MODEL_NAME}/${TIMESTAMP}/
          aws s3api put-object --bucket $BUCKET_NAME --key models/${MODEL_NAME}/latest/
          
          # Upload GGUF model
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/${MODEL_NAME}.gguf
          
          # Upload Modelfile
          aws s3 cp ./Modelfile s3://${BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/Modelfile
          
          # Upload evaluation results
          aws s3 cp ./evaluation_results.json s3://${BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/evaluation_results.json
          
          # Update "latest" pointer
          aws s3 cp ./models/${MODEL_NAME}.gguf s3://${BUCKET_NAME}/models/${MODEL_NAME}/latest/${MODEL_NAME}.gguf
          aws s3 cp ./Modelfile s3://${BUCKET_NAME}/models/${MODEL_NAME}/latest/Modelfile
          aws s3 cp ./evaluation_results.json s3://${BUCKET_NAME}/models/${MODEL_NAME}/latest/evaluation_results.json
          
          echo "Model artifacts uploaded to: s3://${BUCKET_NAME}/models/${MODEL_NAME}/${TIMESTAMP}/"
          echo "Latest model available at: s3://${BUCKET_NAME}/models/${MODEL_NAME}/latest/"

      - name: Generate Report
        run: |
          python scripts/generate_report.py --eval-file ./evaluation_results.json --output-file ./report.md

      - name: Upload Evaluation Results
        uses: actions/upload-artifact@v4
        with:
          name: model-evaluation
          path: |
            ./evaluation_results.json
            ./report.md

      - name: Add Evaluation Summary Comment
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const reportContent = fs.readFileSync('./report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: reportContent
            });