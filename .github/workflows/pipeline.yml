name: Ollama Pipeline with SageMaker

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

jobs:
  build-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Create necessary directories
        run: |
          mkdir -p ollama-data
          mkdir -p scripts/training_scripts
          
      - name: Setup training script
        run: |
          cat > scripts/training_scripts/train.py << 'EOF'
          #!/usr/bin/env python3
          
          import os
          import torch
          from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
          from datasets import load_dataset
          from peft import LoraConfig, get_peft_model
          
          # Load model and tokenizer
          model_name = os.environ.get("model_name_or_path")
          print(f"Loading model: {model_name}")
          
          # Train model
          print("Starting training...")
          # (simplified for demo)
          print("Training complete!")
          EOF
      
      - name: Build and run pipeline
        run: |
          docker compose up -d ollama
          
          echo "Waiting for Ollama to initialize..."
          sleep 30
          
          docker compose up pipeline
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-2
          MODELS_TO_PULL: "llama2 mistral"
          FINETUNE_MODEL: "mistralai/Mistral-7B-v0-1"  # Changed from v0.1 to v0-1
          TRAINING_DATA: "s3://ollama-lora-pipeline/training-data/"
          S3_BUCKET: "ollama-lora-pipeline"
          SAGEMAKER_ROLE_ARN: ${{ secrets.AWS_SAGEMAKER_ROLE_ARN }}
          TRAINING_INSTANCE_TYPE: "ml.m5.4xlarge"
          DEBUG: "true"