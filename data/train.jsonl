{"question": "What is the primary goal of the paper titled 'Lightweight Vision Transformer for Efficient Influenza Virus Subtype Classification via Genomic Image Processing'?", "answer": "To develop a lightweight Vision-Transformer-based method that classifies influenza virus sub-types A, B, C and D directly from DNA sequences while requiring fewer parameters and less computation than existing approaches."}
{"question": "Which influenza virus sub-types does the proposed system classify?", "answer": "Influenza A, Influenza B, Influenza C and Influenza D."}
{"question": "Which four image-encoding techniques are applied to DNA sequences in the study?", "answer": "Grammian Angular Summation Field (GASF), Grammian Angular Difference Field (GADF), Angular Division Grammian Matrix (ADGM) and Grammian Difference Tangent Matrix (GDTM)."}
{"question": "Which coding scheme converts nucleotides to numeric values before image construction?", "answer": "Electron-Ion Interaction Pseudopotential (EIIP) coding."}
{"question": "Give the EIIP values for A, C, G and T.", "answer": "A = 0.1260, C = 0.1340, G = 0.0806, T = 0.1335."}
{"question": "How many DNA sequences were collected and what was their length range?", "answer": "1 000 sequences, each between 1 000 and 1 500 base-pairs long."}
{"question": "Which deep-learning backbones were compared in the experiments?", "answer": "A lightweight Vision Transformer (the proposed method) versus MobileNetV2."}
{"question": "How many trainable parameters does the proposed Vision Transformer have?", "answer": "≈ 3.4 million parameters."}
{"question": "What was the best testing accuracy achieved and with which image-encoding?", "answer": "98.34 % using the GADF image-encoding."}
{"question": "List the testing accuracies of the proposed model for GASF, GADF, ADGM and GDTM.", "answer": "GASF = 98.00 %, GADF = 98.34 %, ADGM = 98.00 %, GDTM = 98.17 %."}
{"question": "Which model showed higher variability in accuracy across encodings?", "answer": "MobileNetV2 showed higher variability (92.83 %–96.00 %)."}
{"question": "What learning-rate and weight-decay were used to train the Vision Transformer?", "answer": "Adam optimiser with a learning-rate of 1 × 10⁻⁴ and weight-decay 0.01."}
{"question": "Why is the proposed model suitable for deployment on IoT devices?", "answer": "Because it couples high accuracy with low parameter count and minimal preprocessing, enabling on-device inference under limited compute budgets."}
{"question": "Which mathematical operation defines a GASF matrix element Gᵢⱼ?", "answer": "Gᵢⱼ = cos(φᵢ + φⱼ), where φᵢ = arccos(xᵢ) for the normalised sequence value xᵢ."}
{"question": "What activation replaces convolution in the Vision Transformer when processing an image?", "answer": "Self-attention layers operating on fixed-size image patches serve the role of convolution in capturing global context."}
